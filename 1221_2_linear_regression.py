# -*- coding: utf-8 -*-
"""실습2_Linear_Regression (1221).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYSBuatfY6jX-cNkac_qsUHAUqJS7cXe

<p style="font-family: Arial; font-size:3.75em;color:black; font-style:bold"><br>
Linear Regression
</p><br>

Reference: https://github.com/leventbass/linear_regression

조교
- 이정수 (KAIST AI 석사과정): bebeto@kaist.ac.kr
- 이상현 (KAIST AI 박사과정): shlee6825@kaist.ac.kr


Boston dataset 설명: https://thatascience.com/learn-machine-learning/boston-dataset/

<span style="font-family:Helvetica ;font-size: 14px; line-height:2.2">
    
## Library 설치 및 dataset 준비
"""

!pip install scikit-learn
!pip install matplotlib

import numpy as np
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt

dataset = load_boston()

X = dataset.data
y = dataset.target[:,np.newaxis] # np.newaxis는 (506,)인것을 (506,1)로 만들기 위함 -> 원래는 row vector로 되어있던 것을 column vector처럼 사용하기 위함 
print("Total samples in our dataset is: {}".format(X.shape[0]))

"""<span style="font-family:Helvetica ;font-size: 14px; line-height:2.2">
    
## 우리가 사용할 함수 선언
"""

def compute_cost(X, y, params):
    n_samples = len(y)
    h = X @ params
    return (1/(2*n_samples))*np.sum((h-y)**2)

"""h: hypothesis를 의미. 즉, $h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$
<br>
cost: $\frac{1}{2N}\Sigma^{N}_{i=1}(h(x_{i})-y_{i})^2$
"""

def gradient_descent(X, y, params, learning_rate, n_iters):
    n_samples = len(y)
    J_history = np.zeros((n_iters,1))
    
    for i in range(n_iters):
        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) 
        J_history[i] = compute_cost(X, y, params)

    return (J_history, params)

"""Gradient update
<br>
$\theta = \theta - \alpha \frac{\partial J(x)}{\partial \theta}$
<br>
$J(x)= \frac{1}{2N}\Sigma^{N}_{i=1}(h(x_{i})-y_{i})^2$
<br>
$J(x)= \frac{1}{2N}[(h(x_1)-y_1)^2 + (h(x_2)-y_2)^2 + (h(x_3)-y_3)^2 + ... + (h(x_n)-y_n)^2)]$
<br>
$\frac{\partial J(x)}{\partial \theta_i} = 2\frac{1}{2N}(h(x_i)-y_i)(x_i) $
"""

n_samples = len(y)
mu = np.mean(X, 0)
sigma = np.std(X, 0)

print(n_samples)
print(mu)
print(sigma)

print('before normalization')
print(X)
X = (X-mu) / sigma # https://www.datasklr.com/ols-least-squares-regression/scaling-centering-and-standardization
print('after normalization: ')
print(X)

X = np.hstack((np.ones((n_samples,1)),X)) # bias term 때문에 1로 구성된 열벡터를 추가
n_features = np.size(X,1)
params = np.zeros((n_features,1))

print(X.shape)
print(params.shape)
print(n_features)

n_iters = 1500
learning_rate = 0.01

initial_cost = compute_cost(X, y, params)

print("Initial cost is: ", initial_cost, "\n")

(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)

print("Optimal parameters are: \n", optimal_params, "\n")

print("Final cost is: ", J_history[-1])

plt.plot(range(len(J_history)), J_history, 'r')
plt.title("Convergence Graph of Cost Function")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost")
plt.show()

"""
## Linear Regression를 클래스화 하기 """

class LinearRegression():
    def __init__(self, X, y, alpha=0.03, n_iter=1500):

        self.alpha = alpha
        self.n_iter = n_iter
        self.n_samples = len(y)
        self.n_features = np.size(X, 1)
        self.X = np.hstack((np.ones((self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))
        self.y = y[:, np.newaxis]
        self.params = np.zeros((self.n_features + 1, 1))
        self.coef_ = None
        self.intercept_ = None

    def fit(self):

        for i in range(self.n_iter):
            self.params = self.params - (self.alpha/self.n_samples) * self.X.T @ (self.X @ self.params - self.y)

        self.intercept_ = self.params[0]
        self.coef_ = self.params[1:]
        return self

    def score(self, X=None, y=None):

        if X is None:
            X = self.X
        else:
            n_samples = np.size(X, 0)
            X = np.hstack((np.ones((n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))

        if y is None:
            y = self.y
        else:
            y = y[:, np.newaxis]

        y_pred = X @ self.params
        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())

        return score

    def predict(self, X):
        n_samples = np.size(X, 0)
        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) / np.std(X, 0))) @ self.params
        return y

    def get_params(self):

        return self.params

from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
import pandas as pd


dataset = load_boston()

X = dataset.data
y = dataset.target

X_train, X_test, y_train, y_test = train_test_split(\
                X, y, test_size=0.3, random_state=42)

regressor = LinearRegression(X_train, y_train).fit()
train_accuracy = regressor.score()
test_accuracy = regressor.score(X_test, y_test)

print('train accuracy: ', train_accuracy)
print('test accuracy: ', test_accuracy)

